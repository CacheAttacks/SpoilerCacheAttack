\chapter{Diskussion}
\label{chapter:discussion}

TODO Netten Einleitungssatz schreiben

%\section{Geschwindigkeit von Eviction-Set Suche}
%Wie in Abschnitt %\ref{} 
%beschrieben, unterliegen die Laufzeiten der Eviction sind

\section{Warum viele Kerne sinnvoll sind}
Da präzise Timer zurzeit mittels eines Shared-Array-Buffer erzeugt werden müssen, benötigt die Angreiferin mindestens drei virtuelle Kerne um ein Angriff auszuführen. Einen für die Iteration der Timer-Variable, einen für den Prime-and-Probe Angriffscode und einen auf dem das Opferprogramm läuft.

Theoretisch reichen deshalb bereits Prozessoren mit zwei physischen und vier virtuellen Kernen wie sie noch vielfach von Intel im Umlauf sind. Bei zwei physischen Kernen wird der Timer-Thread jedoch zeitweise exklusiv auf einen physischen Kern rechnen und sich zeitweise einen Kern mit einem der anderen Threads teilen.
Wenn ein anderer Thread auf dem gleichen Kern rechnet halbiert sich in etwa die Iterationsgeschwindigkeit der Timervariable, sodass die Werte des Timers starken Schwankungen unterliegen.
Problematisch ist dies beispielsweise in der Expand-Phase, da nicht erkannt wird, dass ein Candidate-Set bereits ein Eviction-Set für die Zeugenadresse ist, da die Werte des Timers das Gegenteil suggerieren.

Somit ist es essenziell zu erkennen wann der Timer-Thread ein Kern für sich alleine beansprucht.
Es könnte zur Laufzeit wiederholt nach einer festen Zeitspanne die Timergenauigkeit überprüft werden.
Der Wert der Funktion $performance.now()$ als Referenz heranzuziehen, wie im Kapitel 2 geschehen, ist aufgrund der geringen Auflösung (Chrome etwa 0,1ms) zu langsam.
TODO auf nuc testen.
Als Alternative kann etwa die Laufzeit einer Prime-and-Probe Operation auf einem wenig aktiven Cache-Set als Referenzwert genommen werden.
Hierbei ist zu beachten, dass die Prüfung im Thread des Angriffscodes läuft und dieser sich eventuell auch einen physischen Kern teilen muss.

Ein weiteres Problem ist die Wahl der Zeitspanne, da eine kleine viel Overhead erzeugen würde und eine große zu langsam reagiert, sodass in der Zwischenzeit bereits Timer-Werte falsch interpretiert wurden.

Wie erwähnt steht der Thread des Angriffscodes vor dem gleichen Problem, sodass sich die Dauer eine Prime-and-Probe Iteration im schlimmsten Fall verdoppeln kann.
Folglich sollte ein Angriff auch mit halbierter Geschwindigkeit der Prime-and-Probe Operation erfolgreich sein, da ansonsten Cache-Aktivitäten in bestimmten Zeitabschnitten verloren gehen.

Andersherum wird es im Laufe des Angriffs passieren, dass sich Opferprogramm und Timer einen physischen Kern teilen.
Sofern das Problem der halbierten Timerauflösung gelöst wird, wird die Auflösung des Prime-and-Probe-Angriffs effektiv verdoppelt, da auch das Opferprogramm mit halber Geschwindigkeit läuft.
Da die Zuordnung der Prozesse zu den virtuellen Kernen im Browser nicht beeinflussbar ist und somit aus Sicht der Angreiferin willkürlich erfolgt, werden nur zufällige Zeitabschnitte besser aufgelöst.
Allerdings sollten hier auch die Kosten gegengerechnet werden, die für die Erkennung der veränderten Timerauflösung entstehen.

Aufgrund der beschrieben Probleme und den Auswirkungen der Abmilderungen ebendieser ist es besser auf mindestens drei physische Kerne zurückgreifen zu können.
Sobald die Browserhersteller die Auflösung von $performance.now()$ wieder in den Nanosekundenbereich hieven, wären auch zwei physische Kerne ausreichend. 

Wenn das angegriffene Endgerät $n$ virtuelle Kerne besitzt können also $n-3$ für die Verlangsamung des Opferprogramms eingesetzt werden.

\section{Viel hilft viel?}

Nach den Überlegungen des vorherigen Kapitels sind $n-1-k$ Kerne für die Bremsung nutzbar, wobei $k$ die Anzahl der Kerne für die eigentliche Überwachung ist. Abschnitt \ref{PracticalLeakageAnalysis} hat gezeigt, das s ein Thread die Shift beziehungsweise Subtraktions-Operation nicht ausreichend abbremst.
Daher soll im Folgenden analysiert werden, ob mehrere Threads Vorteile beim Bremsen bringen.
Der Testrechner hat 4 physische und 8 virtuelle Kerne und zu besseren Einschätzung des Effekts wird ausschließlich gebremst und nicht gemessen, das heißt es können bis zu 6 Bremsthreads auf die virtuellen Kerne verteilt werden.
Zu beachten ist das Problem der Threadaufteilung, wenn mindestens 3 und höchstens 5 Bremsthreads verwendet werden.
In diesem Fall werden Threads zeitweise exklusiv auf einem physischen Kern laufen und etwa doppelt so performant sein, wie Threads die sich ihren Kern mit einem anderem Thread teilen müssen.
Da die Zuordnung der Threads zu den Kernen im Browser nicht verändert werden kann, ist unklar welche Threads alleine laufen.

Daher werden die Laufzeitmessungen mehrere Sekunden durchgeführt und anschließend gemittelt, damit der Effekt nicht die Ergebnisse verfälscht.
Der Bremseffekt sollte jedoch bei ab 3 Bremsthreads nachlassen, da sich die Bremsthread dann zeitweise den Kern mit einem anderen Thread teilen müssen.

Des Weiteren tritt ab 4 Bremsthreads eine zusätzliche Verlangsamung durch die mangelnde Anzahl an physischen Kernen ein.
Um diese zu prüfen, wird ein Test mit verschieden vielen Bremsthreads durchgeführt, wobei die Bremsthreads keine Codezeilen der Shift- oder Subtraktions-Operation bremsen.
Tabelle \ref{} zeigt, dass erst ab 4 Bremsthreads ein signifikanter Performancerückgang messbar ist.

%Warum ist das eigentlich so?
%Bei 3 Bremsthreads, einem Counter-Thread, sowie dem aktiven Opferprogramm werden insgesamt 5 Threads, bei nur 4 phsischen Kernen, genutzt. 
%Dennoch tritt gegenüber 2 Bremsthreads reproduzierbar keine messbare Verlangsamung ein.
%


Die für die Bremsung wurden die Cache-Lines verwendet, welche in den Tests in Abschnitt \ref{PracticalLeakageAnalysis} (siehe Tabelle \ref{tbl:PerformanceDegShift} und \ref{tbl:PerformanceDegSub}) am besten bremsten.

TODO Benchmarkergebnisse ausformulieren. Betonen, dass nicht mehrere Cache-Line pro Thread genutzt werden sollten (Überraschend, da beim Bremsthread mehrere Cache-Lines besser). 

%V1: 1: 's_mp_clamp_0', //s_mp_clamp>: 541,608,549k
%V2: 2:  33: 's_mp_clamp_1', //538,605,553k
%V10:4:  58: 's_mp_div_2d_2', //539,557,551k
%V11: 5:  59: 's_mp_div_2d_3', //537,547,552k 536,551,557k
%V6: 3:  48: 's_mp_rshd_3', //520,571,597k  500,536,548k, 509,543,562k
%V8: 6:  56: 's_mp_div_2d_0', //s_mp_div_2d>: 511,560, 544k  526,564,542k



%Tabelle \ref{} zeigt, den Effekt wenn mehrere Threads für den Bremsvorgang verwendet werden.


%opferprozess läuft die ganze zeit in eigenem prozess
%chrome dafür alle bremsthreads auch in einem prozess

%ref: 340,475,430k
%1t:347,477,443k
%2t:361,516,467k
%3t:373,505,467k
%4t:435,558,546k
%5t:448,574,556k
%6t:474,601,598k

%c ref threads:
%ref:341,486,436k
%3:359,494,454k
%4:366,504,461k
%5:410,531,511k
%6:438,585,544k
%7:486,583,594k

%-O0:
%3:358,489,450k
%4:362,494,454k
%5:454,582,567k

%TODO multithread bench

\section{Warum OpenPGP.js nicht anfällig ist}

Montgomery Implementation von gcd beschreiben

OpenPGPjs gcd(p-1,e) im Gegensatz zu Mozilla NSS schon bei Primzahlgenerierung

https://github.com/openpgpjs/openpgpjs/blob/master/src/crypto/public_key/rsa.js

pow (Zeile 3332) scheinbar fixed window implementation:\\
https://github.com/indutny/bn.js/blob/e69c617b3297b99aca429f30842e27979ef9beb5/lib/bn.js

\section{Memory-locking: Mit Kanonen auf Spatzen schießen}
\label{MemoryLocking}

Als Memory-locking soll im Folgenden ein Ansatz bezeichnet werden, welcher die Ausführungsgeschwindigkeit von Prozesses verlangsamt in dem der Zugriff auf den Hauptspeicher wiederholt kurzzeitig blockiert wird.
Beschrieben und verwendet wurde diese Technik in \cite{MemoryLockingWu, MemoryLockingRisenpart, MemoryLockingJavaAndroid}.
Um die kurzzeitige Blockierung des Memory-Buses zu erzwingen wird atomar auf eine Variable geschrieben, die in zwei Cache-Lines liegt.

Atomare Operationen verursachen systemweite Blockierungen in den Speicherregionen auf denen sie arbeiten, wobei diese Blockierungen in Prozessoren nicht immer lokal begrenzt sind.
Die ersten Generationen von x86-Prozessoren blockierten bei einer atomaren Operation noch den gesamten Speicherbus.
Dies sorgt jedoch für Performanceeinbußen da ein Befehl alle Speicherzugriffe, die etwa aufgrund von Out-of-Order-Execution parallel ausgeführt werden könnten, blockiert.
Des Weiteren skaliert dieser Ansatz schlecht mit Mehrkernprozessoren, da andere Kerne während der atomaren Operation auf Speicherzugriffe verzichten müssten.

Häufig arbeiten atomare Operationen auf Speicherbereichen die in eine Cache-Line passen. Diese Eigenschaft nutzen x86-Prozessoren ab dem Pentium Pro aus, indem sie nur die zugehörige Cache-Line sperren und nicht mehr den gesamten Speicherbus.
Auf diesen Systemen kann es dennoch zur vorübergehenden Sperrung des gesamten Speicherbusses kommen, etwa wenn der Speicher für die atomare Operation nicht alignt ist und sich über zwei Cache-Lines spannt.

x86-Prozessoren ab dem Intel Nehalem und AMD K8/K10 verwenden anders als die vorherigen Generationen keinen gemeinsamen Speicherbus mehr. Stattdessen ist der Speicher aufgeteilt und jedem Kern ein Bereich als lokaler Speicher zugeordnet, auch Non-Uniform Memory Access   (NUMA) genannt. 
Die Kerne besitzen direkten Zugriff auf ihren Speicherbereich, wobei auf die Bereiche der anderen Kerne über einen gemeinsamen Adressraum zugegriffen werden kann.
Wenn die Daten einer atomare Operationen innerhalb einer Cache-Line liegen, wird hier analog zu den vorherigen Generationen nur diese Cache-Line gesperrt.
Bei einer atomaren Operation dessen Daten sich über zwei Cache-Lines spannen, stimmen sich aufgrund des fehlenden gemeinsamen Speicherbusses alle Prozessoren ab, um ihre aktuell in Ausführung befindenden Speichertransaktionen zu flushen.
Dies entspricht also einer Simulation für das Sperren des kompletten Speicherbusses.

Diese Verhalten kann nun bewusst provoziert werden, indem atomare Operationen auf Daten zwischen zwei Cache-Lines ausgeführt werden.
Dadurch werden alle Speicherzugriffe aller Programme gebremst, in der Hoffnung, dass sich das Opferprogramm stärker als das Angriffsprogramm verlangsamt.

TODO Beschreiben warum nicht geeignet und Benchmarkergenisse ausformulieren

%perf deg in c ohne zusätze:
%ref:341, 480, 432k
%1t:373,522,469
%1t:370,522,470
%2t:361,506,455
%3t:392,542,479
%4t:797,986,977k


%test c programm mit clflush verdoppelt laufzeit (referenz-zeit=105):
%1t:195-230
%2t:191-300
%3t:230-500
%4t:280-750
%5t:220-960
  %  uint64_t start = rdtscp64();
  %  for (int i=1;i<=dongu;i++){
  %    clflush(&temp);
  %    temp2 = temp;
  %    }

%Verlansamung von Prime-and-probe Operation von 500ns auf 930ns wenn Aktivität auf Cache-set

%ref: 348,473,438k
%prime spam v1 only: 539, 600, 566k
%deg 1t only:370,510,469k
%prime spam v1 und deg 1t: 894,527,594k
%prime spam v1 und deg 2t: 628,527,526k(vermutlich wegen 5 threads insgesamt, reproduzierbar langsamer als 1t deg)

%deg 2t only: 388, 521, 488k
%deg 2+ eher unnötig, da ab 4threads ganzer pc rumlahmt

%prime spam v1(clamp0),v6(rshd3) only: 964,787,865k

%prime spam v1(clamp0),v6(rshd3) und deg 1t: 1337,596,743k
%(problem hierbei einzelene operationen dauern bis zu 3000 taktzyklen)
%prime spam v1(clamp0),v6(rshd3) und deg 2t: 1262,575,701k
%(problem hierbei einzelene operationen dauern bis zu 2000 taktzyklen, weniger krasse peaks als bei 1t)

%fazit threads besser in bremsthread stecken, anstatt in deg


%multithread bremsen:
%problem bremsen verursacht peaks bei shift ketten:
%13360848296169248 1224 -1 
%13360848296169583 335 -1 
%13360848296170242 659 -1 
%aber zu langsam um meherere am stück zu bremsen


%deg 1t und prime spam 2t(v1 und v10): 1312,680,803k

%deg 1t und prime spam 3t(v1 und v6 und v10): 1462,636,837k
%(peaks über 2000 nicht so krass wie bei prime spam v1(clamp0),v6(rshd3) und deg 1t
%hierbei messen 3070ns pro p-and-p

%measure p-and-p: 500 bei aktivität, 
%900 bei aktivität und 1t deg
%(!!!)370ns bei keiner aktivität und 1t deg

%zeige diagram auf cache-set ohne aktivität bei laufendem deg vs nicht laufendem deg





%TODO benchmarks einfügen

%Risenpart:
%http://pages.cs.wisc.edu/~venkatv/pvstudy-usenixsecurity15.pdf


%Wu:
%https://www.usenix.org/system/files/conference/usenixsecurity12/sec12-final97.pdf


\section{Wie viel schneller muss er sein?}

Abschnitt \ref{} und \ref{MemoryLocking}

Beschreiben bei wie vielen Taktzyklen Pausen mp_gcd ausgelesen werden kann.

\section{Speicherallokation: Wie hilfsbereit ist das Betriebssystem?}

Wenn der Speicher wenig fragmentiert ist, etwa nach dem Booten und große zusammenhängende physischer Speicherbereiche frei sind, scheint Ubuntu 16.04.5 LTS dahin zu tendieren das $evictionBuffer$-Array in diese Bereiche zu mappen.
Angenommen der komplette evictionBuffer wird in einen zusammenhängeden physischen Speicherbereich gemappt. Dann ist zu erwarten, dass nach jeweils $2^8$ Speicherblöcken eine neue colliding address gefunden wird, da $2^8$ Speicherblöcke genau $2^20$ Bytes belegen und bei einer colliding address die letzten 20 Bit identisch sind.

Die ersten Versuche fanden statt, nachdem der Testrechner schon über 30 Minuten in Betrieb war und viele Tests und Programme ausgeführt wurden.
Wenn sich unter diesen Bedingungen der Abstand der ersten 10 Speicherblöcke zueinander angeschaut wird, welche eine colliding address zum Speicherblock 0 bilden, zeigen sich Abstandsfolgen wie 
\begin{align*} 
654, 938, 1438, 224, 372, 543, 464, 84, 38, 134\\
170, 236, 573, 66, 452, 124, 206, 228, 1026, 62
\end{align*}
Diese scheinen auch nach näherer Betrachtung rein willkürlich um den Erwartungswert 256 zu schwanken.

Interessant werden die Folgen wenn der Testrechner neu gestartet wird und der physische Adressbereich somit wenig fragmentiert ist.
Dann können Abstandsfolgen wie 
\begin{align*} 
256, 256, 256, 353, 256, 256, 258, 254, 256, 188\\
256, 256, 256, 256, 256, 256, 271, 256, 256, 256
\end{align*}
beobachtet werden.
Bei den gegebenen Beispielen sind vereinzelt von 256 abweichende Abstände zu beobachten, die scheinbar nicht aus einem Fehler bei der colliding address Erkennung resultieren.

Denn angenommen die Abweichungen stammen aus vereinzelten Fehlern bei der Erkennung, dann sollten Abstandsfolgen wie 256, 353, 159, 256 entstehen mit $353+159=2*256$.
Die beobachteten Folgen jedoch weisen nach der vereinzelten Abweichung häufig wieder den Abstand 256 auf, sodass die Abweichung kein Fehler in der Erkennung zu sein scheint.

Ein weiteres Indiz dafür liefert die erfolgreiche Verifikation, dass der Speicherblock mit dem abweichenden Abstand später Teil eines Eviction-Sets wird.

Die längste ununterbrochene Folge mit dem optimalen Abstand 256 war bei allen Tests immer auf den Wert 10 beschränkt, wobei etwa 60-80\% aller Abstände den optimalen Wert 256 aufweisen.
Somit scheint die Größe zusammenhängender physischer Speicherbereiche aus dem Autor nicht bekannten Gründen begrenzt zu sein.

Mit dieser Beobachtung lässt sich 

Bermerkungen Mem-Alloc von Firefox/Chrome: https://www.cs.vu.nl/~giuffrida/papers/anc-ndss-2017.pdf
\cite{ASLROnTheLine}

\section{Warum ist Google so mutig?}
\label{GooglePageIsolation}

Wie erwähnt hat Google bereits Anfang August 2018 mit der Chrome Version 68 \cite{ChromeSharedArrayBufferAgain} die SharedArrayBuffer standardmäßig wieder aktiviert.
Mozilla hingegen hat auch in der aktuellen Firefox Developer Edition 63.0b8 (Stand Ende September 2018) die SharedArrayBuffer in der Standardeinstellung deaktiviert.

Google begründet seinen Schritt mit der Einführung der Site Isolation \cite{ChromeSiteIsolation}, welche jeden Renderprozess auf Dokumente einer Webseite beschränkt. Somit ist der Inhalt jeder Webseite in einem eigenen Prozess gekapselt und Chrome kann auf die Sicherheitsmaßnahmen des Betriebssystem zurückgreifen.
Chrome besaß bereits vor der Version 68 eine Multiprozessarchitektur in der Tabs auf unterschiedliche Prozesse aufgeteilt wurden.
Jedoch war es vorher möglich, dass eine vertrauenswürdige Webseite und ein darin enthaltender Iframe mit Inhalten einer bösartigen Webseite im selben Renderprozess landeten.
In diesem Fall wäre ein Spectre-Angriff möglich, in welchem Daten wie Cookies oder Passwörter von der vertrauenswürdigen Seite abgeschöpft werden könnten.
Der Unterschied lässt sich auch in der Praxis leicht anhand einer typischen mit Drittanbieterwerbung versehenen Webseite wie Spiegel online nachvollziehen.
Chromium 67.0.3396.0 startet einen zusätzlichen Prozess, Chromium 68.0.3440.0 hingegen 6 weitere Prozesse, um Spiegel Online mit all seiner Werbung in einem neuen Tab darzustellen.

%https://omahaproxy.appspot.com/
%https://commondatastorage.googleapis.com/chromium-browser-snapshots/index.html?prefix=Win_x64/550416/
%vs
%https://commondatastorage.googleapis.com/chromium-browser-snapshots/index.html?prefix=Win_x64/594476/

Mozilla arbeitet ebenfalls an Site-Isolation, wird aber vermutlich noch Monate brauchen, um dieses Feature in die Release-Version von Firefox zu integrieren \cite{FirefoxSiteIsolation}.
Die aktuelle Firefox Developer Edition 63.0b8 teilt in der Standardeinstellung alle Webseiteninhalte auf vier Content-Prozesse auf. 
Somit kann wie oben beschrieben mittels eines Spectre Angriffs auf alle Webseitendaten desjenigen Content-Prozesses zugegriffen werden, in dem die Webseite mit dem Angriffscode liegt.
Tests haben ergeben, dass Firefox die Tabs nach dem Round-Robin-Verfahren auf die Content-Prozesse aufteilt, womit eine erfolgreiche Angreiferin die Webseitendaten von einem Viertel der offenen Tabs abgreifen könnte.

Aus diesem Grund wird Mozilla in Firefox vermutlich erst nach der Einführung der Site Isolation Änderungen bezüglich der Verfügbarkeit hochpräsizer Timer vornehmen.

\section{Wie wohl fühlen sich Meltdown und Spectre im Browser?}
\label{MeltdownSpectreBrowser}

https://alephsecurity.com/2018/06/26/spectre-browser-query-cache/
\cite{OvercomingSpectreBrowserMitigations}

Sofern Meltdown über Prozessgrenzen hinweg Speicher auslesen soll, muss der geschützte Speicher in den Adressebereich des Angriffsprozess gemappt werden.
Dies ist im Browser nicht ohne weiteres möglich, da Webassembly eine eigene Speicheradressierung mitbringt.
Auch Spectre

\section{Triviale Divsionen: Ist das sinnvoll oder kann das Weg?}

-Benchmarks von Mozilla NSS und OpenPGPjs ohne Divisionstests

\section{Javascript vs. Webassembly}

Vergleiche Javascript/webassembly bei prime and probe geschwindigkeit